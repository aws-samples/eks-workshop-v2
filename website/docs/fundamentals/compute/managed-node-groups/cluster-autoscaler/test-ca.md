---
title: "Scale with CA"
sidebar_position: 40
---

In this section we'll update all of the application components to increase their replica count to 4. This will cause more resources to be consumed than are available in a cluster, triggering more compute to be provisioned.

```file
manifests/modules/autoscaling/compute/cluster-autoscaler/deployment.yaml
```

Let's apply this to our cluster:

```bash hook=ca-pod-scaleout timeout=180
$ kubectl apply -k ~/environment/eks-workshop/modules/autoscaling/compute/cluster-autoscaler
```

Some pods will be in the `Pending` state, which triggers the cluster-autoscaler to scale out the EC2 fleet.

```bash test=false
$ kubectl get pods -A -o wide --watch
```

View the cluster-autoscaler logs:

```bash test=false
$ kubectl -n kube-system logs \
  -f deployment/cluster-autoscaler-aws-cluster-autoscaler
```

You would see similar to following logs generated by cluster-autoscaler indicating a scale-out event to add a new node:

```text
...
...
I0411 21:26:52.108599       1 klogx.go:87] Pod ui/ui-68495c748c-dbh22 is unschedulable
I0411 21:26:52.108604       1 klogx.go:87] Pod ui/ui-68495c748c-98gcq is unschedulable
I0411 21:26:52.108608       1 klogx.go:87] Pod ui/ui-68495c748c-8pkdv is unschedulable
I0411 21:26:52.108903       1 orchestrator.go:108] Upcoming 0 nodes
I0411 21:26:52.109318       1 orchestrator.go:181] Best option to resize: eks-default-62c766f6-ec38-5423-ce6a-c4633f142631
I0411 21:26:52.109334       1 orchestrator.go:185] Estimated 1 nodes needed in eks-default-62c766f6-ec38-5423-ce6a-c4633f142631
I0411 21:26:52.109358       1 orchestrator.go:291] Final scale-up plan: [{eks-default-62c766f6-ec38-5423-ce6a-c4633f142631 3->4 (max: 6)}]
I0411 21:26:52.109376       1 executor.go:147] Scale-up: setting group eks-default-62c766f6-ec38-5423-ce6a-c4633f142631 size to 4
I0411 21:26:52.109428       1 auto_scaling_groups.go:267] Setting asg eks-default-62c766f6-ec38-5423-ce6a-c4633f142631 size to 4
...
...
```

Check the [EC2 AWS Management Console](https://console.aws.amazon.com/ec2/home?#Instances:sort=instanceId) to confirm that the Auto Scaling groups are scaling up to meet demand. This may take a few minutes. You can also follow along with the pod deployment from the command line. You should see the pods transition from pending to running as nodes are scaled up.

Alternatively you can use `kubectl`:

```bash
$ kubectl get nodes -l workshop-default=yes
NAME                                         STATUS   ROLES    AGE     VERSION
ip-10-42-10-159.us-west-2.compute.internal   Ready    <none>   3d      vVAR::KUBERNETES_NODE_VERSION
ip-10-42-11-143.us-west-2.compute.internal   Ready    <none>   3d      vVAR::KUBERNETES_NODE_VERSION
ip-10-42-11-81.us-west-2.compute.internal    Ready    <none>   3d      vVAR::KUBERNETES_NODE_VERSION
ip-10-42-12-152.us-west-2.compute.internal   Ready    <none>   3m11s   vVAR::KUBERNETES_NODE_VERSION
```
