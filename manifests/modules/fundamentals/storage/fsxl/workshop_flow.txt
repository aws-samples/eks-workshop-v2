1. Prepare environment (wait ~6 minutes for FSx Lustre file system to be created)

prepare-environment fundamentals/storage/fsxl


2.  Command script to create data repository association and notify user when created (~8 min)
    Copy whole section below at once to do

ASSOCIATION_ID=$(aws fsx create-data-repository-association \
    --file-system-id $FSX_ID \
    --file-system-path "/" \
    --data-repository-path "s3://$BUCKET_NAME" \
    --s3 "AutoImportPolicy={Events=[NEW,CHANGED,DELETED]},AutoExportPolicy={Events=[NEW,CHANGED,DELETED]}" \
    --query 'Association.AssociationId' \
    --output text)

echo "Creating Data Repository Association..."

while true; do
    STATUS=$(aws fsx describe-data-repository-associations --association-ids $ASSOCIATION_ID --query 'Associations[0].Lifecycle' --output text)
    
    if [ "$STATUS" = "AVAILABLE" ]; then
        echo "Data Repository Association is now AVAILABLE."
        break
    elif [ "$STATUS" = "FAILED" ]; then
        echo "Data Repository Association creation FAILED."
        break
    fi
    sleep 5
done


3. Put files into S3 bucket

mkdir ~/environment/assets-images/
cd ~/environment/assets-images/
curl --remote-name-all https://raw.githubusercontent.com/aws-containers/retail-store-sample-app/main/src/assets/public/assets/{chrono_classic.jpg,gentleman.jpg,pocket_watch.jpg,smart_2.jpg,wood_watch.jpg}
cd ~/environment/
aws s3 cp ~/environment/assets-images/ s3://$BUCKET_NAME/ --recursive
aws s3 ls $BUCKET_NAME

4. Helm use for CSI Driver installation

helm repo add aws-fsx-csi-driver https://kubernetes-sigs.github.io/aws-fsx-csi-driver/

helm upgrade --install aws-fsx-csi-driver \
    --namespace kube-system \
    aws-fsx-csi-driver/aws-fsx-csi-driver

5. Verify installation
kubectl get daemonset fsx-csi-node -n kube-system

6. Statically provision FSx for Lustre volume in EKS cluster
kubectl kustomize ~/environment/eks-workshop/modules/fundamentals/storage/fsxl/deployment \
  | envsubst | kubectl apply -f-

7. Check status of deployment
kubectl rollout status --timeout=130s deployment/assets -n assets

8. Check mounts on deployment
kubectl get deployment -n assets \
  -o yaml | yq '.items[].spec.template.spec.containers[].volumeMounts'

9. Check PV
kubectl get pv

10. Check PVC in asseets namespace
kubectl describe pvc -n assets

11. Show that contents from S3 bucket, imported into FSx Lustre and now in pods
POD_NAME=$(kubectl -n assets get pods -o jsonpath='{.items[0].metadata.name}')
kubectl exec --stdin $POD_NAME -n assets -- bash -c 'ls /fsx-lustre/'

12. Put divewatch.png in S3 bucket
touch divewatch.png && aws s3 cp divewatch.png s3://$BUCKET_NAME/

13. Now verify that this file exists in the first Pod
kubectl exec --stdin $POD_NAME -n assets -- bash -c 'ls /fsx-lustre/'

14. Now verify that this file exists in the second Pod
POD_NAME=$(kubectl -n assets get pods -o jsonpath='{.items[1].metadata.name}')
kubectl exec --stdin $POD_NAME -n assets -- bash -c 'ls /fsx-lustre/'

Explanation: Pods are sharing storage with FSx Lustre file system that is synced with S3 bucket so
as files added they are propogated to FSx Lustre and pod mounts