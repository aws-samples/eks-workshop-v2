#!/bin/bash

mkdir -p /eks-workshop/logs
log_file=/eks-workshop/logs/action-$(date +%s).log

exec 7>&1

logmessage() {
  echo "$@" >&7
  echo "$@" >&1
}
export -f logmessage

if [ -z "${DEV_MODE}" ]; then
  # Redirection for logging
  exec >$log_file 2> >(tee >(cat >&7))
else
  # Log the commands in dev mode
  set -o xtrace
fi

if [ -z "$EKS_CLUSTER_NAME" ]; then
  logmessage "Error: The EKS_CLUSTER_NAME environment variable must be set. Please run 'use-cluster <your cluster name>'"
  exit 1
fi

module=$1

repository_path="/eks-workshop/repository"
manifests_path="/eks-workshop/manifests"
base_path="$manifests_path/base-application"

set -Eeuo pipefail
trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    logmessage "An error occurred, please contact your workshop proctor or raise an issue at https://github.com/aws-samples/eks-workshop-v2/issues"
    logmessage "The full log can be found here: $log_file"
  fi
  exec 3<&-
}

mkdir -p /eks-workshop

rm -f /home/ec2-user/environment/eks-workshop

rm -rf $manifests_path

REPOSITORY_REF=${REPOSITORY_REF:-""}

if [ ! -z "${REPOSITORY_REF}" ]; then
  rm -rf $repository_path

  logmessage "Refreshing copy of workshop repository from GitHub..."

  git clone --quiet https://github.com/$REPOSITORY_OWNER/$REPOSITORY_NAME.git $repository_path
  (cd $repository_path && git checkout --quiet "${REPOSITORY_REF}")

  logmessage ""

  cp -R $repository_path/manifests $manifests_path
elif [ -d "/manifests" ]; then
  cp -R /manifests $manifests_path
fi

ln -s $manifests_path /home/ec2-user/environment/eks-workshop

if [ ! -z "$module" ]; then
  if [ $module = "introduction/getting-started" ]; then
    exit
  fi
fi

logmessage "Resetting the environment..."
logmessage "Tip: Read the rest of the lab introduction while you wait!"

if [ -f "/eks-workshop/hooks/cleanup.sh" ]; then
  bash /eks-workshop/hooks/cleanup.sh
fi

kubectl delete pod load-generator --ignore-not-found

kubectl delete namespace other --ignore-not-found

kubectl apply -k $base_path --prune --all \
  --prune-allowlist=autoscaling/v1/HorizontalPodAutoscaler \
  --prune-allowlist=core/v1/Service \
  --prune-allowlist=core/v1/ConfigMap \
  --prune-allowlist=apps/v1/Deployment \
  --prune-allowlist=apps/v1/StatefulSet \
  --prune-allowlist=core/v1/ServiceAccount \
  --prune-allowlist=core/v1/Secret \
  --prune-allowlist=core/v1/PersistentVolumeClaim \
  --prune-allowlist=scheduling.k8s.io/v1/PriorityClass \
  --prune-allowlist=networking.k8s.io/v1/Ingress

logmessage "Waiting for application to become ready..."

sleep 10

kubectl wait --for=condition=available --timeout=240s deployments -l app.kubernetes.io/created-by=eks-workshop -A
kubectl wait --for=condition=Ready --timeout=240s pods -l app.kubernetes.io/created-by=eks-workshop -A

# Addons
mkdir -p /eks-workshop/terraform
cp $manifests_path/.workshop/terraform/base.tf /eks-workshop/terraform

export TF_VAR_eks_cluster_id="$EKS_CLUSTER_NAME"

RESOURCES_PRECREATED=${RESOURCES_PRECREATED:-""}

logmessage "Cleaning up previous lab infrastructure..."

tf_dir=$(realpath --relative-to="$PWD" '/eks-workshop/terraform')

terraform -chdir="$tf_dir" init -upgrade
terraform -chdir="$tf_dir" destroy --auto-approve

rm -rf /eks-workshop/terraform/addon*.tf

rm -rf /eks-workshop/hooks

if [ ! -z "$module" ]; then
  module_path="$manifests_path/modules/$module"

  if [ -f "$module_path/.workshop/cleanup.sh" ]; then
    mkdir -p /eks-workshop/hooks
    cp "$module_path/.workshop/cleanup.sh" /eks-workshop/hooks
  fi

  if [ -f "$module_path/.workshop/terraform/addon.tf" ]; then
    logmessage "Creating infrastructure for next lab..."

    cp -R $module_path/.workshop/terraform/* /eks-workshop/terraform

    if [ "$RESOURCES_PRECREATED" = "true" ]; then
      rm -f /eks-workshop/terraform/addon_infrastructure.tf
    fi

    terraform -chdir="$tf_dir" init -upgrade
    terraform -chdir="$tf_dir" apply -refresh=false --auto-approve
  fi

  if [ -d "$module_path/.workshop/manifests" ]; then
    kubectl apply -k "$module_path/.workshop/manifests"
  fi
fi

terraform -chdir="$tf_dir" output -json | jq -r '.environment.value | select(. != null)' > ~/.bashrc.d/workshop-env.bash

# Node groups
expected_size_config="$EKS_DEFAULT_MNG_MIN $EKS_DEFAULT_MNG_MAX $EKS_DEFAULT_MNG_DESIRED"

mng_size_config=$(aws eks describe-nodegroup --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME | jq -r '.nodegroup.scalingConfig | "\(.minSize) \(.maxSize) \(.desiredSize)"')

if [[ "$mng_size_config" != "$expected_size_config" ]]; then
  logmessage "Setting EKS Node Group back to initial sizing..."

  aws eks update-nodegroup-config --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME \
    --scaling-config desiredSize=$EKS_DEFAULT_MNG_DESIRED,minSize=$EKS_DEFAULT_MNG_MIN,maxSize=$EKS_DEFAULT_MNG_MAX
  aws eks wait nodegroup-active --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME

  sleep 10
fi

asg_size_config=$(aws autoscaling describe-auto-scaling-groups --filters "Name=tag:eks:nodegroup-name,Values=$EKS_DEFAULT_MNG_NAME" "Name=tag:eks:cluster-name,Values=$EKS_CLUSTER_NAME" | jq -r '.AutoScalingGroups[0] | "\(.MinSize) \(.MaxSize) \(.DesiredCapacity)"')

if [[ "$asg_size_config" != "$expected_size_config" ]]; then
  logmessage "Setting ASG back to initial sizing..."

  export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --filters "Name=tag:eks:nodegroup-name,Values=$EKS_DEFAULT_MNG_NAME" "Name=tag:eks:cluster-name,Values=$EKS_CLUSTER_NAME" --query "AutoScalingGroups[0].AutoScalingGroupName" --output text)
  aws autoscaling update-auto-scaling-group \
      --auto-scaling-group-name $ASG_NAME \
      --min-size $EKS_DEFAULT_MNG_MIN \
      --max-size $EKS_DEFAULT_MNG_MAX \
      --desired-capacity $EKS_DEFAULT_MNG_DESIRED
fi

EXIT_CODE=0

timeout -s TERM 300 bash -c \
    'while [[ $(kubectl get nodes -l workshop-default=yes -o json | jq -r ".items | length") -gt 3 ]];\
    do sleep 30;\
    done' || EXIT_CODE=$?

if [ $EXIT_CODE -ne 0 ]; then
  >&2 echo "Error: Nodes did not scale back to 3"
  exit 1
fi

# Recycle workload pods in case stateful pods got restarted
kubectl delete pod -l app.kubernetes.io/created-by=eks-workshop -l app.kubernetes.io/component=service -A

kubectl wait --for=condition=Ready --timeout=240s pods -l app.kubernetes.io/created-by=eks-workshop -A

# Finished
logmessage 'Environment is ready'
