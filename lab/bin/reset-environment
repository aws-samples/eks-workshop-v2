#!/bin/bash

mkdir -p /eks-workshop/logs
log_file=/eks-workshop/logs/action-$(date +%s).log

exec 7>&1

logmessage() {
  echo "$@" >&7
  echo "$@" >&1
}
export -f logmessage

if [ -z "${DEV_MODE}" ]; then
  # Redirection for logging
  exec >$log_file 2> >(tee >(cat >&7))
else
  # Log the commands in dev mode
  set -o xtrace
fi

if [ -z "$EKS_DEFAULT_MNG_MIN" ]; then
  EKS_CLUSTER_NAME=${EKS_CLUSTER_NAME:-"eks-workshop"}

  logmessage "Error: Please run 'use-cluster $EKS_CLUSTER_NAME'"
  exit 1
fi

rm -f /home/ec2-user/.kube/config

aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --alias default --user-alias default

module=$1

repository_path="/eks-workshop/repository"
manifests_path="/eks-workshop/manifests"
base_path="$manifests_path/base-application"

set -Eeuo pipefail
trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    logmessage "An error occurred, please contact your workshop proctor or raise an issue at https://github.com/aws-samples/eks-workshop-v2/issues"
    logmessage "The full log can be found here: $log_file"
  fi
  exec 3<&-
}

mkdir -p /eks-workshop

rm -f /home/ec2-user/environment/eks-workshop

rm -rf $manifests_path

REPOSITORY_REF=${REPOSITORY_REF:-""}

if [ ! -z "${REPOSITORY_REF}" ]; then
  rm -rf $repository_path

  logmessage "Refreshing copy of workshop repository from GitHub..."

  git clone --depth=1 --single-branch --branch=${REPOSITORY_REF} --quiet https://github.com/$REPOSITORY_OWNER/$REPOSITORY_NAME.git $repository_path
  
  logmessage ""

  cp -R $repository_path/manifests $manifests_path
elif [ -d "/manifests" ]; then
  cp -R /manifests $manifests_path
fi

ln -s $manifests_path /home/ec2-user/environment/eks-workshop

if [ ! -z "$module" ]; then
  ANALYTICS_ENDPOINT=${ANALYTICS_ENDPOINT:-""}

  if [ ! -z "$ANALYTICS_ENDPOINT" ]; then
    AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
    curl --get -s --data-urlencode "lab=$module" --data-urlencode "account_id=$AWS_ACCOUNT_ID" $ANALYTICS_ENDPOINT || true
  fi

  if [ $module = "introduction/getting-started" ]; then
    exit
  fi
fi

logmessage "Resetting the environment..."
logmessage "Tip: Read the rest of the lab introduction while you wait!"

if [ -f "/eks-workshop/hooks/cleanup.sh" ]; then
  bash /eks-workshop/hooks/cleanup.sh
fi

kubectl delete pod load-generator --ignore-not-found

kubectl delete namespace other --ignore-not-found

kubectl apply -k $base_path --prune --all \
  --prune-allowlist=autoscaling/v1/HorizontalPodAutoscaler \
  --prune-allowlist=core/v1/Service \
  --prune-allowlist=core/v1/ConfigMap \
  --prune-allowlist=apps/v1/Deployment \
  --prune-allowlist=apps/v1/StatefulSet \
  --prune-allowlist=core/v1/ServiceAccount \
  --prune-allowlist=core/v1/Secret \
  --prune-allowlist=core/v1/PersistentVolumeClaim \
  --prune-allowlist=scheduling.k8s.io/v1/PriorityClass \
  --prune-allowlist=networking.k8s.io/v1/Ingress

logmessage "Waiting for application to become ready..."

sleep 10

kubectl wait --for=condition=available --timeout=240s deployments -l app.kubernetes.io/created-by=eks-workshop -A
kubectl wait --for=condition=Ready --timeout=240s pods -l app.kubernetes.io/created-by=eks-workshop -A

# Addons
rm -rf /eks-workshop/terraform
mkdir -p /eks-workshop/terraform
cp -R $manifests_path/.workshop/terraform/* /eks-workshop/terraform

export TF_VAR_eks_cluster_id="$EKS_CLUSTER_NAME"

RESOURCES_PRECREATED=${RESOURCES_PRECREATED:-"false"}

logmessage "Cleaning up previous lab infrastructure..."

tf_dir=$(realpath --relative-to="$PWD" '/eks-workshop/terraform')

terraform -chdir="$tf_dir" init -upgrade
terraform -chdir="$tf_dir" destroy --auto-approve

rm -rf /eks-workshop/hooks

if [ ! -z "$module" ]; then
  module_path="$manifests_path/modules/$module"

  if [ -f "$module_path/.workshop/cleanup.sh" ]; then
    mkdir -p /eks-workshop/hooks
    cp "$module_path/.workshop/cleanup.sh" /eks-workshop/hooks
  fi

  if [ -f "$module_path/.workshop/terraform/main.tf" ]; then
    logmessage "Creating infrastructure for next lab..."

    cp -R $module_path/.workshop/terraform/* /eks-workshop/terraform/lab

    export TF_VAR_resources_precreated="$RESOURCES_PRECREATED"

    terraform -chdir="$tf_dir" init -upgrade
    terraform -chdir="$tf_dir" apply -refresh=false --auto-approve
  elif [ -f "$module_path/.workshop/terraform/addon.tf" ]; then
    # This is the deprecated legacy code path that will be removed
    logmessage "Creating infrastructure for next lab..."

    cp -R $module_path/.workshop/terraform/* /eks-workshop/terraform

    if [ "$RESOURCES_PRECREATED" = "true" ]; then
      rm -f /eks-workshop/terraform/addon_infrastructure.tf
    fi

    terraform -chdir="$tf_dir" init -upgrade
    terraform -chdir="$tf_dir" apply -refresh=false --auto-approve
  fi

  if [ -d "$module_path/.workshop/manifests" ]; then
    kubectl apply -k "$module_path/.workshop/manifests"
  fi
fi

terraform -chdir="$tf_dir" output -json | jq -r '.environment.value | select(. != null)' > ~/.bashrc.d/workshop-env.bash

# Node groups
expected_size_config="$EKS_DEFAULT_MNG_MIN $EKS_DEFAULT_MNG_MAX $EKS_DEFAULT_MNG_DESIRED"

mng_size_config=$(aws eks describe-nodegroup --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME | jq -r '.nodegroup.scalingConfig | "\(.minSize) \(.maxSize) \(.desiredSize)"')

if [[ "$mng_size_config" != "$expected_size_config" ]]; then
  logmessage "Setting EKS Node Group back to initial sizing..."

  WAIT_EXIT_CODE=0

  # Wait for the node group to be active in case previous module cleanup didn't complete for some reason
  aws eks wait nodegroup-active --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME || WAIT_EXIT_CODE=$?

  if [ $WAIT_EXIT_CODE -ne 0 ]; then
    logmessage "Default node group is still being modified unexpectedly, please try again."
    logmessage "If this error persists please contact your workshop proctor or raise an issue at https://github.com/aws-samples/eks-workshop-v2/issues"
    exit 1
  fi

  aws eks update-nodegroup-config --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME \
    --scaling-config desiredSize=$EKS_DEFAULT_MNG_DESIRED,minSize=$EKS_DEFAULT_MNG_MIN,maxSize=$EKS_DEFAULT_MNG_MAX
  aws eks wait nodegroup-active --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME

  sleep 10
fi

asg_size_config=$(aws autoscaling describe-auto-scaling-groups --filters "Name=tag:eks:nodegroup-name,Values=$EKS_DEFAULT_MNG_NAME" "Name=tag:eks:cluster-name,Values=$EKS_CLUSTER_NAME" | jq -r '.AutoScalingGroups[0] | "\(.MinSize) \(.MaxSize) \(.DesiredCapacity)"')

if [[ "$asg_size_config" != "$expected_size_config" ]]; then
  logmessage "Setting ASG back to initial sizing..."

  export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --filters "Name=tag:eks:nodegroup-name,Values=$EKS_DEFAULT_MNG_NAME" "Name=tag:eks:cluster-name,Values=$EKS_CLUSTER_NAME" --query "AutoScalingGroups[0].AutoScalingGroupName" --output text)
  aws autoscaling update-auto-scaling-group \
      --auto-scaling-group-name $ASG_NAME \
      --min-size $EKS_DEFAULT_MNG_MIN \
      --max-size $EKS_DEFAULT_MNG_MAX \
      --desired-capacity $EKS_DEFAULT_MNG_DESIRED
fi

EXIT_CODE=0

timeout -s TERM 300 bash -c \
    'while [[ $(kubectl get nodes -l workshop-default=yes -o json | jq -r ".items | length") -gt 3 ]];\
    do sleep 30;\
    done' || EXIT_CODE=$?

if [ $EXIT_CODE -ne 0 ]; then
  >&2 echo "Error: Nodes did not scale back to 3"
  exit 1
fi

# Recycle workload pods in case stateful pods got restarted
kubectl delete pod -l app.kubernetes.io/created-by=eks-workshop -l app.kubernetes.io/component=service -A

kubectl wait --for=condition=Ready --timeout=240s pods -l app.kubernetes.io/created-by=eks-workshop -A

# Finished
logmessage 'Environment is ready'
